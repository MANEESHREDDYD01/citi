{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864df1f-3b61-4d10-b63e-a0e0bb7a2acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded: ..\\data\\raw\\202401-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202402-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202403-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202404-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202405-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202406-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202407-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202408-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202409-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202410-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202411-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202412-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202501-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202502-citibike-tripdata.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202503-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: ..\\data\\raw\\202504-citibike-tripdata.zip\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "def download_all_zip_files():\n",
    "    # List of confirmed file names based on the screenshot\n",
    "    file_names = [\n",
    "        \"202401-citibike-tripdata.csv.zip\",\n",
    "        \"202402-citibike-tripdata.csv.zip\",\n",
    "        \"202403-citibike-tripdata.csv.zip\",\n",
    "        \"202404-citibike-tripdata.csv.zip\",\n",
    "        \"202405-citibike-tripdata.zip\",\n",
    "        \"202406-citibike-tripdata.zip\",\n",
    "        \"202407-citibike-tripdata.zip\",\n",
    "        \"202408-citibike-tripdata.zip\",\n",
    "        \"202409-citibike-tripdata.zip\",\n",
    "        \"202410-citibike-tripdata.zip\",\n",
    "        \"202411-citibike-tripdata.zip\",\n",
    "        \"202412-citibike-tripdata.zip\",\n",
    "        \"202501-citibike-tripdata.zip\",\n",
    "        \"202502-citibike-tripdata.zip\",\n",
    "        \"202503-citibike-tripdata.csv.zip\"\n",
    "    ]\n",
    "\n",
    "    for fname in file_names:\n",
    "        url = f\"https://s3.amazonaws.com/tripdata/{fname}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            path = Path(\"..\") / \"data\" / \"raw\" / fname\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            path.write_bytes(response.content)\n",
    "            print(f\"✅ Downloaded: {path}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed: {url}\")\n",
    "\n",
    "# Run the function\n",
    "download_all_zip_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab943f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "def fetch_raw_data(year: int, month: int) -> str:\n",
    "    fname_zip = f\"{year}{month:02}-citibike-tripdata.csv.zip\"\n",
    "    alt_fname_zip = f\"{year}{month:02}-citibike-tripdata.zip\"\n",
    "    raw_dir = Path(\"..\") / \"data\" / \"raw\"\n",
    "    unzip_dir = Path(\"..\") / \"data\" / \"unzipped\"\n",
    "    unzip_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Choose correct file format\n",
    "    zip_path = raw_dir / fname_zip\n",
    "    if not zip_path.exists():\n",
    "        zip_path = raw_dir / alt_fname_zip\n",
    "        if not zip_path.exists():\n",
    "            print(f\"❌ Not found: {fname_zip} or {alt_fname_zip}\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_dir)\n",
    "            for file in zip_ref.namelist():\n",
    "                if file.endswith(\".csv\"):\n",
    "                    return str(unzip_dir / file)\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ Corrupted ZIP: {zip_path}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50b6309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Processing: 2024-01\n",
      "📦 Processing: 2024-02\n",
      "📦 Processing: 2024-03\n",
      "📦 Processing: 2024-04\n",
      "📦 Processing: 2024-05\n",
      "📦 Processing: 2024-06\n",
      "📦 Processing: 2024-07\n",
      "📦 Processing: 2024-08\n",
      "📦 Processing: 2024-09\n",
      "📦 Processing: 2024-10\n",
      "📦 Processing: 2024-11\n",
      "📦 Processing: 2024-12\n",
      "📦 Processing: 2025-01\n",
      "📦 Processing: 2025-02\n",
      "📦 Processing: 2025-03\n",
      "✅ Total rows combined: 21023102\n",
      "            ride_id  rideable_type               started_at  \\\n",
      "0  5078F3D302000BD2  electric_bike  2024-01-22 18:43:19.012   \n",
      "1  814337105D37302A  electric_bike  2024-01-11 19:19:18.721   \n",
      "2  A33A920E2B10710C  electric_bike  2024-01-30 19:17:41.693   \n",
      "3  A3A5FC0DD7D34D74  electric_bike  2024-01-27 11:27:01.759   \n",
      "4  6F96728ECEFBDAA4  electric_bike  2024-01-16 15:15:41.000   \n",
      "\n",
      "                  ended_at                  start_station_name  \\\n",
      "0  2024-01-22 18:48:10.708  Frederick Douglass Blvd & W 145 St   \n",
      "1  2024-01-11 19:47:36.007                     W 54 St & 6 Ave   \n",
      "2  2024-01-30 19:32:49.857                     E 11 St & Ave B   \n",
      "3  2024-01-27 11:38:01.213                     W 54 St & 6 Ave   \n",
      "4  2024-01-16 15:29:26.156               Madison Ave & E 99 St   \n",
      "\n",
      "  start_station_id            end_station_name end_station_id  start_lat  \\\n",
      "0          7954.12  St Nicholas Ave & W 126 St        7756.10  40.823072   \n",
      "1          6771.13             E 74 St & 1 Ave        6953.08  40.761822   \n",
      "2          5659.11     W 10 St & Washington St        5847.06  40.727592   \n",
      "3          6771.13             E 74 St & 1 Ave        6953.08  40.761779   \n",
      "4          7443.01             E 74 St & 1 Ave        6953.08  40.789808   \n",
      "\n",
      "   start_lng    end_lat    end_lng member_casual  Unnamed: 0  \\\n",
      "0 -73.941738  40.811432 -73.951878        member         NaN   \n",
      "1 -73.977036  40.768974 -73.954823        member         NaN   \n",
      "2 -73.979751  40.733424 -74.008515        casual         NaN   \n",
      "3 -73.977144  40.768974 -73.954823        member         NaN   \n",
      "4 -73.952214  40.768974 -73.954823        member         NaN   \n",
      "\n",
      "  rideable_type_duplicate_column_name_1  \n",
      "0                                   NaN  \n",
      "1                                   NaN  \n",
      "2                                   NaN  \n",
      "3                                   NaN  \n",
      "4                                   NaN  \n"
     ]
    }
   ],
   "source": [
    "def load_all_data(start_year=2024, end_year=2025, end_month=3):\n",
    "    all_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            if year == end_year and month > end_month:\n",
    "                break\n",
    "            print(f\"📦 Processing: {year}-{month:02}\")\n",
    "            csv_path = fetch_raw_data(year, month)\n",
    "            if csv_path:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path, dtype={\"start_station_id\": str, \"end_station_id\": str})\n",
    "                    all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error reading {csv_path}: {e}\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Load and combine all data\n",
    "combined_df = load_all_data()\n",
    "print(f\"✅ Total rows combined: {len(combined_df)}\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Processing month: 2024_01 with 1 files\n",
      "  🔄 Reading: 202401-citibike-tripdata.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_01.parquet with 1888085 rows\n",
      "\n",
      "📦 Processing month: 2024_02 with 1 files\n",
      "  🔄 Reading: 202402-citibike-tripdata.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_02.parquet with 2121501 rows\n",
      "\n",
      "📦 Processing month: 2024_03 with 1 files\n",
      "  🔄 Reading: 202403-citibike-tripdata.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_03.parquet with 2663295 rows\n",
      "\n",
      "📦 Processing month: 2024_04 with 1 files\n",
      "  🔄 Reading: 202404-citibike-tripdata.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_04.parquet with 3217063 rows\n",
      "\n",
      "📦 Processing month: 2024_05 with 5 files\n",
      "  🔄 Reading: 202405-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202405-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202405-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202405-citibike-tripdata_4.csv\n",
      "  🔄 Reading: 202405-citibike-tripdata_5.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_05.parquet with 4230360 rows\n",
      "\n",
      "📦 Processing month: 2024_06 with 5 files\n",
      "  🔄 Reading: 202406-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202406-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202406-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202406-citibike-tripdata_4.csv\n",
      "  🔄 Reading: 202406-citibike-tripdata_5.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_06.parquet with 4783576 rows\n",
      "\n",
      "📦 Processing month: 2024_07 with 5 files\n",
      "  🔄 Reading: 202407-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202407-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202407-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202407-citibike-tripdata_4.csv\n",
      "  🔄 Reading: 202407-citibike-tripdata_5.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_07.parquet with 4722896 rows\n",
      "\n",
      "📦 Processing month: 2024_08 with 5 files\n",
      "  🔄 Reading: 202408-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202408-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202408-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202408-citibike-tripdata_4.csv\n",
      "  🔄 Reading: 202408-citibike-tripdata_5.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_08.parquet with 4603575 rows\n",
      "\n",
      "📦 Processing month: 2024_09 with 5 files\n",
      "  🔄 Reading: 202409-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202409-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202409-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202409-citibike-tripdata_4.csv\n",
      "  🔄 Reading: 202409-citibike-tripdata_5.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_09.parquet with 4997898 rows\n",
      "\n",
      "📦 Processing month: 2024_11 with 4 files\n",
      "  🔄 Reading: 202411-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202411-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202411-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202411-citibike-tripdata_4.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_11.parquet with 3710134 rows\n",
      "\n",
      "📦 Processing month: 2024_12 with 3 files\n",
      "  🔄 Reading: 202412-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202412-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202412-citibike-tripdata_3.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_12.parquet with 2311171 rows\n",
      "\n",
      "📦 Processing month: 2025_01 with 3 files\n",
      "  🔄 Reading: 202501-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202501-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202501-citibike-tripdata_3.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2025_01.parquet with 2124475 rows\n",
      "\n",
      "📦 Processing month: 2025_02 with 3 files\n",
      "  🔄 Reading: 202502-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202502-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202502-citibike-tripdata_3.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2025_02.parquet with 2031257 rows\n",
      "\n",
      "📦 Processing month: 2025_03 with 1 files\n",
      "  🔄 Reading: 202503-citibike-tripdata.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2025_03.parquet with 3168271 rows\n",
      "\n",
      "📦 Processing month: 2024_10 with 6 files\n",
      "  🔄 Reading: 202410-citibike-tripdata_1.csv\n",
      "  🔄 Reading: 202410-citibike-tripdata_2.csv\n",
      "  🔄 Reading: 202410-citibike-tripdata_3.csv\n",
      "  🔄 Reading: 202410-citibike-tripdata_4.csv\n",
      "  🔄 Reading: 202410-citibike-tripdata_5.csv\n",
      "  🔄 Reading: 202410-citibike-tripdata_6.csv\n",
      "✅ Saved: ..\\data\\processed\\monthly\\rides_2024_10.parquet with 5150054 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def save_monthly_files_with_chunks(unzip_dir=\"../data/unzipped\", output_dir=\"../data/processed/monthly\"):\n",
    "    unzip_path = Path(unzip_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    monthly_files = {}\n",
    "\n",
    "    # Group files by month using regex\n",
    "    for file in unzip_path.rglob(\"*.csv\"):\n",
    "        if file.name.startswith(\"._\"):  # Skip macOS system files\n",
    "            continue\n",
    "        match = re.search(r\"(20\\d{2})(\\d{2})\", file.name)\n",
    "        if match:\n",
    "            year, month = match.groups()\n",
    "            key = f\"{year}_{month}\"\n",
    "            monthly_files.setdefault(key, []).append(file)\n",
    "\n",
    "    for key, file_list in monthly_files.items():\n",
    "        print(f\"\\n📦 Processing month: {key} with {len(file_list)} files\")\n",
    "        monthly_data = []\n",
    "\n",
    "        for file in file_list:\n",
    "            try:\n",
    "                print(f\"  🔄 Reading: {file.name}\")\n",
    "                chunks = pd.read_csv(file, low_memory=False, dtype={\"start_station_id\": str, \"end_station_id\": str}, chunksize=500_000)\n",
    "                for chunk in chunks:\n",
    "                    monthly_data.append(chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipped {file.name} due to error: {e}\")\n",
    "\n",
    "        if monthly_data:\n",
    "            combined = pd.concat(monthly_data, ignore_index=True)\n",
    "            output_file = output_path / f\"rides_{key}.parquet\"\n",
    "            combined.to_parquet(output_file, index=False)\n",
    "            print(f\"✅ Saved: {output_file} with {len(combined)} rows\")\n",
    "        else:\n",
    "            print(f\"🚫 No valid data for {key}\")\n",
    "\n",
    "# Run the improved function\n",
    "save_monthly_files_with_chunks()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
